{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnext50.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN/XA7JGj2wAv9m825X1Xx+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moh2236945/pytorch_classification/blob/master/models/resnext50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i1d6yW6xfNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\"\"\"\n",
        "NOTICE:\n",
        "    BasicBlock_B is not implemented\n",
        "    BasicBlock_C is recommendation\n",
        "    The full architecture consist of BasicBlock_A is not implemented.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "class ResBottleBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_planes, bottleneck_width=4, stride=1, expansion=1):\n",
        "        super(ResBottleBlock, self).__init__()\n",
        "        self.conv0=nn.Conv2d(in_planes,bottleneck_width,1,stride=1,bias=False)\n",
        "        self.bn0 = nn.BatchNorm2d(bottleneck_width)\n",
        "        self.conv1=nn.Conv2d(bottleneck_width,bottleneck_width,3,stride=stride,padding=1,bias=False)\n",
        "        self.bn1=nn.BatchNorm2d(bottleneck_width)\n",
        "        self.conv2=nn.Conv2d(bottleneck_width,expansion*in_planes,1,bias=False)\n",
        "        self.bn2=nn.BatchNorm2d(expansion*in_planes)\n",
        "        \n",
        "        self.shortcut=nn.Sequential()\n",
        "        if stride!=1 or expansion!=1:\n",
        "            self.shortcut=nn.Sequential(\n",
        "                nn.Conv2d(in_planes,in_planes*expansion,1,stride=stride,bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn0(self.conv0(x)))\n",
        "        out = F.relu(self.bn1(self.conv1(out)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BasicBlock_A(nn.Module):\n",
        "    def __init__(self, in_planes, num_paths=32, bottleneck_width=4, expansion=1, stride=1):\n",
        "        super(BasicBlock_A,self).__init__()\n",
        "        self.num_paths = num_paths\n",
        "        for i in range(num_paths):\n",
        "            setattr(self,'path'+str(i),self._make_path(in_planes,bottleneck_width,stride,expansion))\n",
        "\n",
        "        # self.paths=self._make_path(in_planes,bottleneck_width,stride,expansion)\n",
        "        self.conv0=nn.Conv2d(in_planes*expansion,expansion*in_planes,1,stride=1,bias=False)\n",
        "        self.bn0 = nn.BatchNorm2d(in_planes * expansion)\n",
        "        \n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or expansion != 1:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, in_planes * expansion, 1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.path0(x)\n",
        "        for i in range(1,self.num_paths):\n",
        "            if hasattr(self,'path'+str(i)):\n",
        "                out+getattr(self,'path'+str(i))(x)\n",
        "            # out+=self.paths(x)\n",
        "            # getattr\n",
        "        # out = torch.sum(out, dim=1)\n",
        "        out = self.bn0(out)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "    def _make_path(self, in_planes, bottleneck_width, stride, expansion):\n",
        "        layers = []\n",
        "        layers.append(ResBottleBlock(\n",
        "            in_planes, bottleneck_width, stride, expansion))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlock_C(nn.Module):\n",
        "    \"\"\"\n",
        "    increasing cardinality is a more effective way of \n",
        "    gaining accuracy than going deeper or wider\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_planes, bottleneck_width=4, cardinality=32, stride=1, expansion=2):\n",
        "        super(BasicBlock_C, self).__init__()\n",
        "        inner_width = cardinality * bottleneck_width\n",
        "        self.expansion = expansion\n",
        "        self.basic = nn.Sequential(OrderedDict(\n",
        "            [\n",
        "                ('conv1_0', nn.Conv2d(in_planes, inner_width, 1, stride=1, bias=False)),\n",
        "                ('bn1', nn.BatchNorm2d(inner_width)),\n",
        "                ('act0', nn.ReLU()),\n",
        "                ('conv3_0', nn.Conv2d(inner_width, inner_width, 3, stride=stride, padding=1, groups=cardinality, bias=False)),\n",
        "                ('bn2', nn.BatchNorm2d(inner_width)),\n",
        "                ('act1', nn.ReLU()),\n",
        "                ('conv1_1', nn.Conv2d(inner_width, inner_width * self.expansion, 1, stride=1, bias=False)),\n",
        "                ('bn3', nn.BatchNorm2d(inner_width * self.expansion))\n",
        "            ]\n",
        "        ))\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != inner_width * self.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, inner_width * self.expansion, 1, stride=stride, bias=False)\n",
        "            )\n",
        "        self.bn0 = nn.BatchNorm2d(self.expansion * inner_width)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.basic(x)\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(self.bn0(out))\n",
        "        return out\n",
        "\n",
        "class ResNeXt(nn.Module):\n",
        "    def __init__(self, num_blocks, cardinality, bottleneck_width, expansion=2, num_classes=10):\n",
        "        super(ResNeXt, self).__init__()\n",
        "        self.cardinality = cardinality\n",
        "        self.bottleneck_width = bottleneck_width\n",
        "        self.in_planes = 64\n",
        "        self.expansion = expansion\n",
        "        \n",
        "        self.conv0 = nn.Conv2d(3, self.in_planes, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn0 = nn.BatchNorm2d(self.in_planes)\n",
        "        self.pool0 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1=self._make_layer(num_blocks[0],1)\n",
        "        self.layer2=self._make_layer(num_blocks[1],2)\n",
        "        self.layer3=self._make_layer(num_blocks[2],2)\n",
        "        self.layer4=self._make_layer(num_blocks[3],2)\n",
        "        self.linear = nn.Linear(self.cardinality * self.bottleneck_width, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn0(self.conv0(x)))\n",
        "        # out = self.pool0(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layer(self, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock_C(self.in_planes, self.bottleneck_width, self.cardinality, stride, self.expansion))\n",
        "            self.in_planes = self.expansion * self.bottleneck_width * self.cardinality\n",
        "        self.bottleneck_width *= 2\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def resnext26_2x64d():\n",
        "    return ResNeXt(num_blocks=[2, 2, 2, 2], cardinality=2, bottleneck_width=64)\n",
        "\n",
        "\n",
        "def resnext26_4x32d():\n",
        "    return ResNeXt(num_blocks=[2, 2, 2, 2], cardinality=4, bottleneck_width=32)\n",
        "\n",
        "\n",
        "def resnext26_8x16d():\n",
        "    return ResNeXt(num_blocks=[2, 2, 2, 2], cardinality=8, bottleneck_width=16)\n",
        "\n",
        "\n",
        "def resnext26_16x8d():\n",
        "    return ResNeXt(num_blocks=[2, 2, 2, 2], cardinality=16, bottleneck_width=8)\n",
        "\n",
        "\n",
        "def resnext26_32x4d():\n",
        "    return ResNeXt(num_blocks=[2, 2, 2, 2], cardinality=32, bottleneck_width=4)\n",
        "\n",
        "\n",
        "def resnext26_64x2d():\n",
        "    return ResNeXt(num_blocks=[2, 2, 2, 2], cardinality=32, bottleneck_width=4)\n",
        "\n",
        "\n",
        "def resnext50_2x64d():\n",
        "    return ResNeXt(num_blocks=[3, 4, 6, 3], cardinality=2, bottleneck_width=64)\n",
        "\n",
        "\n",
        "def resnext50_32x4d():\n",
        "    return ResNeXt(num_blocks=[3, 4, 6, 3], cardinality=32, bottleneck_width=4)\n",
        "\n",
        "# def test():\n",
        "#     net = resnext50_2x64d()\n",
        "#     # print(net)\n",
        "#     data = Variable(torch.rand(1, 3, 32, 32))\n",
        "#     output = net(data)\n",
        "#     print(output.size())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCnWUy2GxuZz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e4d6f93-44d6-4dbd-e3af-5a8704febffa"
      },
      "source": [
        "model = resnext50_2x64d()\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "summary(model,(3, 32, 32))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3          [-1, 128, 32, 32]           8,192\n",
            "       BatchNorm2d-4          [-1, 128, 32, 32]             256\n",
            "              ReLU-5          [-1, 128, 32, 32]               0\n",
            "            Conv2d-6          [-1, 128, 32, 32]          73,728\n",
            "       BatchNorm2d-7          [-1, 128, 32, 32]             256\n",
            "              ReLU-8          [-1, 128, 32, 32]               0\n",
            "            Conv2d-9          [-1, 256, 32, 32]          32,768\n",
            "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
            "           Conv2d-11          [-1, 256, 32, 32]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 32, 32]             512\n",
            "     BasicBlock_C-13          [-1, 256, 32, 32]               0\n",
            "           Conv2d-14          [-1, 128, 32, 32]          32,768\n",
            "      BatchNorm2d-15          [-1, 128, 32, 32]             256\n",
            "             ReLU-16          [-1, 128, 32, 32]               0\n",
            "           Conv2d-17          [-1, 128, 32, 32]          73,728\n",
            "      BatchNorm2d-18          [-1, 128, 32, 32]             256\n",
            "             ReLU-19          [-1, 128, 32, 32]               0\n",
            "           Conv2d-20          [-1, 256, 32, 32]          32,768\n",
            "      BatchNorm2d-21          [-1, 256, 32, 32]             512\n",
            "      BatchNorm2d-22          [-1, 256, 32, 32]             512\n",
            "     BasicBlock_C-23          [-1, 256, 32, 32]               0\n",
            "           Conv2d-24          [-1, 128, 32, 32]          32,768\n",
            "      BatchNorm2d-25          [-1, 128, 32, 32]             256\n",
            "             ReLU-26          [-1, 128, 32, 32]               0\n",
            "           Conv2d-27          [-1, 128, 32, 32]          73,728\n",
            "      BatchNorm2d-28          [-1, 128, 32, 32]             256\n",
            "             ReLU-29          [-1, 128, 32, 32]               0\n",
            "           Conv2d-30          [-1, 256, 32, 32]          32,768\n",
            "      BatchNorm2d-31          [-1, 256, 32, 32]             512\n",
            "      BatchNorm2d-32          [-1, 256, 32, 32]             512\n",
            "     BasicBlock_C-33          [-1, 256, 32, 32]               0\n",
            "           Conv2d-34          [-1, 256, 32, 32]          65,536\n",
            "      BatchNorm2d-35          [-1, 256, 32, 32]             512\n",
            "             ReLU-36          [-1, 256, 32, 32]               0\n",
            "           Conv2d-37          [-1, 256, 16, 16]         294,912\n",
            "      BatchNorm2d-38          [-1, 256, 16, 16]             512\n",
            "             ReLU-39          [-1, 256, 16, 16]               0\n",
            "           Conv2d-40          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-41          [-1, 512, 16, 16]           1,024\n",
            "           Conv2d-42          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-43          [-1, 512, 16, 16]           1,024\n",
            "     BasicBlock_C-44          [-1, 512, 16, 16]               0\n",
            "           Conv2d-45          [-1, 256, 16, 16]         131,072\n",
            "      BatchNorm2d-46          [-1, 256, 16, 16]             512\n",
            "             ReLU-47          [-1, 256, 16, 16]               0\n",
            "           Conv2d-48          [-1, 256, 16, 16]         294,912\n",
            "      BatchNorm2d-49          [-1, 256, 16, 16]             512\n",
            "             ReLU-50          [-1, 256, 16, 16]               0\n",
            "           Conv2d-51          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-52          [-1, 512, 16, 16]           1,024\n",
            "      BatchNorm2d-53          [-1, 512, 16, 16]           1,024\n",
            "     BasicBlock_C-54          [-1, 512, 16, 16]               0\n",
            "           Conv2d-55          [-1, 256, 16, 16]         131,072\n",
            "      BatchNorm2d-56          [-1, 256, 16, 16]             512\n",
            "             ReLU-57          [-1, 256, 16, 16]               0\n",
            "           Conv2d-58          [-1, 256, 16, 16]         294,912\n",
            "      BatchNorm2d-59          [-1, 256, 16, 16]             512\n",
            "             ReLU-60          [-1, 256, 16, 16]               0\n",
            "           Conv2d-61          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-62          [-1, 512, 16, 16]           1,024\n",
            "      BatchNorm2d-63          [-1, 512, 16, 16]           1,024\n",
            "     BasicBlock_C-64          [-1, 512, 16, 16]               0\n",
            "           Conv2d-65          [-1, 256, 16, 16]         131,072\n",
            "      BatchNorm2d-66          [-1, 256, 16, 16]             512\n",
            "             ReLU-67          [-1, 256, 16, 16]               0\n",
            "           Conv2d-68          [-1, 256, 16, 16]         294,912\n",
            "      BatchNorm2d-69          [-1, 256, 16, 16]             512\n",
            "             ReLU-70          [-1, 256, 16, 16]               0\n",
            "           Conv2d-71          [-1, 512, 16, 16]         131,072\n",
            "      BatchNorm2d-72          [-1, 512, 16, 16]           1,024\n",
            "      BatchNorm2d-73          [-1, 512, 16, 16]           1,024\n",
            "     BasicBlock_C-74          [-1, 512, 16, 16]               0\n",
            "           Conv2d-75          [-1, 512, 16, 16]         262,144\n",
            "      BatchNorm2d-76          [-1, 512, 16, 16]           1,024\n",
            "             ReLU-77          [-1, 512, 16, 16]               0\n",
            "           Conv2d-78            [-1, 512, 8, 8]       1,179,648\n",
            "      BatchNorm2d-79            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-80            [-1, 512, 8, 8]               0\n",
            "           Conv2d-81           [-1, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-82           [-1, 1024, 8, 8]           2,048\n",
            "           Conv2d-83           [-1, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-84           [-1, 1024, 8, 8]           2,048\n",
            "     BasicBlock_C-85           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-86            [-1, 512, 8, 8]         524,288\n",
            "      BatchNorm2d-87            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-88            [-1, 512, 8, 8]               0\n",
            "           Conv2d-89            [-1, 512, 8, 8]       1,179,648\n",
            "      BatchNorm2d-90            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-91            [-1, 512, 8, 8]               0\n",
            "           Conv2d-92           [-1, 1024, 8, 8]         524,288\n",
            "      BatchNorm2d-93           [-1, 1024, 8, 8]           2,048\n",
            "      BatchNorm2d-94           [-1, 1024, 8, 8]           2,048\n",
            "     BasicBlock_C-95           [-1, 1024, 8, 8]               0\n",
            "           Conv2d-96            [-1, 512, 8, 8]         524,288\n",
            "      BatchNorm2d-97            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-98            [-1, 512, 8, 8]               0\n",
            "           Conv2d-99            [-1, 512, 8, 8]       1,179,648\n",
            "     BatchNorm2d-100            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-101            [-1, 512, 8, 8]               0\n",
            "          Conv2d-102           [-1, 1024, 8, 8]         524,288\n",
            "     BatchNorm2d-103           [-1, 1024, 8, 8]           2,048\n",
            "     BatchNorm2d-104           [-1, 1024, 8, 8]           2,048\n",
            "    BasicBlock_C-105           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-106            [-1, 512, 8, 8]         524,288\n",
            "     BatchNorm2d-107            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-108            [-1, 512, 8, 8]               0\n",
            "          Conv2d-109            [-1, 512, 8, 8]       1,179,648\n",
            "     BatchNorm2d-110            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-111            [-1, 512, 8, 8]               0\n",
            "          Conv2d-112           [-1, 1024, 8, 8]         524,288\n",
            "     BatchNorm2d-113           [-1, 1024, 8, 8]           2,048\n",
            "     BatchNorm2d-114           [-1, 1024, 8, 8]           2,048\n",
            "    BasicBlock_C-115           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-116            [-1, 512, 8, 8]         524,288\n",
            "     BatchNorm2d-117            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-118            [-1, 512, 8, 8]               0\n",
            "          Conv2d-119            [-1, 512, 8, 8]       1,179,648\n",
            "     BatchNorm2d-120            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-121            [-1, 512, 8, 8]               0\n",
            "          Conv2d-122           [-1, 1024, 8, 8]         524,288\n",
            "     BatchNorm2d-123           [-1, 1024, 8, 8]           2,048\n",
            "     BatchNorm2d-124           [-1, 1024, 8, 8]           2,048\n",
            "    BasicBlock_C-125           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-126            [-1, 512, 8, 8]         524,288\n",
            "     BatchNorm2d-127            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-128            [-1, 512, 8, 8]               0\n",
            "          Conv2d-129            [-1, 512, 8, 8]       1,179,648\n",
            "     BatchNorm2d-130            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-131            [-1, 512, 8, 8]               0\n",
            "          Conv2d-132           [-1, 1024, 8, 8]         524,288\n",
            "     BatchNorm2d-133           [-1, 1024, 8, 8]           2,048\n",
            "     BatchNorm2d-134           [-1, 1024, 8, 8]           2,048\n",
            "    BasicBlock_C-135           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-136           [-1, 1024, 8, 8]       1,048,576\n",
            "     BatchNorm2d-137           [-1, 1024, 8, 8]           2,048\n",
            "            ReLU-138           [-1, 1024, 8, 8]               0\n",
            "          Conv2d-139           [-1, 1024, 4, 4]       4,718,592\n",
            "     BatchNorm2d-140           [-1, 1024, 4, 4]           2,048\n",
            "            ReLU-141           [-1, 1024, 4, 4]               0\n",
            "          Conv2d-142           [-1, 2048, 4, 4]       2,097,152\n",
            "     BatchNorm2d-143           [-1, 2048, 4, 4]           4,096\n",
            "          Conv2d-144           [-1, 2048, 4, 4]       2,097,152\n",
            "     BatchNorm2d-145           [-1, 2048, 4, 4]           4,096\n",
            "    BasicBlock_C-146           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-147           [-1, 1024, 4, 4]       2,097,152\n",
            "     BatchNorm2d-148           [-1, 1024, 4, 4]           2,048\n",
            "            ReLU-149           [-1, 1024, 4, 4]               0\n",
            "          Conv2d-150           [-1, 1024, 4, 4]       4,718,592\n",
            "     BatchNorm2d-151           [-1, 1024, 4, 4]           2,048\n",
            "            ReLU-152           [-1, 1024, 4, 4]               0\n",
            "          Conv2d-153           [-1, 2048, 4, 4]       2,097,152\n",
            "     BatchNorm2d-154           [-1, 2048, 4, 4]           4,096\n",
            "     BatchNorm2d-155           [-1, 2048, 4, 4]           4,096\n",
            "    BasicBlock_C-156           [-1, 2048, 4, 4]               0\n",
            "          Conv2d-157           [-1, 1024, 4, 4]       2,097,152\n",
            "     BatchNorm2d-158           [-1, 1024, 4, 4]           2,048\n",
            "            ReLU-159           [-1, 1024, 4, 4]               0\n",
            "          Conv2d-160           [-1, 1024, 4, 4]       4,718,592\n",
            "     BatchNorm2d-161           [-1, 1024, 4, 4]           2,048\n",
            "            ReLU-162           [-1, 1024, 4, 4]               0\n",
            "          Conv2d-163           [-1, 2048, 4, 4]       2,097,152\n",
            "     BatchNorm2d-164           [-1, 2048, 4, 4]           4,096\n",
            "     BatchNorm2d-165           [-1, 2048, 4, 4]           4,096\n",
            "    BasicBlock_C-166           [-1, 2048, 4, 4]               0\n",
            "          Linear-167                   [-1, 10]          20,490\n",
            "================================================================\n",
            "Total params: 44,235,146\n",
            "Trainable params: 44,235,146\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 108.88\n",
            "Params size (MB): 168.74\n",
            "Estimated Total Size (MB): 277.63\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}