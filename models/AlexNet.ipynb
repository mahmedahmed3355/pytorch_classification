{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlexNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP75x3C/jt0dBYOMszAO2HW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moh2236945/pytorch_classification/blob/master/models/AlexNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NN4PmLQhvif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Basic Library\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.functional as F\n",
        "from torch.utils import data\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nXmTX7JjXVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_EPOCHS=90\n",
        "Batch_size=128\n",
        "MOMENTUM=0.9\n",
        "LR_DECCAY=0.005\n",
        "LR_INIT=0.01\n",
        "IMAGE_DIM=227\n",
        "NUM_ClASSES=1000\n",
        "DEVICE_IDS=[0,1,2,3]\n",
        "INPUT_ROOT_DIR='data_in'\n",
        "OUTPUT_DIR='alexnet_data_out'\n",
        "LOG_DIR=OUTPUT_DIR+'/logs'\n",
        "CHECKPOINT_DIR=OUTPUT_DIR+'/models'\n",
        "#checkpoint path directory\n",
        "os.makedirs(CHECKPOINT_DIR,exist_ok=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpYe4GYBwuAX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "97515b9b-d235-4222-dfec-853cd474aa75"
      },
      "source": [
        "class AlexNet(nn.Module):\n",
        "    def __init(self,num_classes=1000):\n",
        "        super().__init__()\n",
        "        self.net=nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3,out_channels=96,kerenel_size=11,stride=4),\n",
        "            nn.Relu(),\n",
        "            nn.LocalResponsenorm(size=5,alpha=0.0001,beta=0.75,k=2),\n",
        "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
        "            nn.Conv2d(96,256,5,padding=2),\n",
        "            nn.Relu(),\n",
        "            nn.localResponsenorm(size=5,aplpha=0.0001,beta=0.75,k=2),\n",
        "            nn.Maxpool2d(kernel_size=3,stride=2),\n",
        "            nn.Conv2d(256,384,3,padding=1),\n",
        "            nn.Relu(),\n",
        "            nn.Conv2d(384,256,3,padding=1),\n",
        "            nn.Relu()\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "        self.classifier=nn.Sequential(\n",
        "            nn.Dropout(p=0.5,inplace=True),\n",
        "            nn.Linear(in_features=(256*6*6),out_features=4096),\n",
        "            nn.Relu(),\n",
        "            nn.Dropout(p=0.5,inplace=True),\n",
        "            nn.Linear(in_features=4096,out_features=4096),\n",
        "            nn.Relu(),\n",
        "            nn.Linear(in_features=4096,out_features=num_classes),\n",
        "        )\n",
        "        self.init_bias()# initialize the bias\n",
        "    def init_bias(self):\n",
        "        for layers in self.net:\n",
        "            if isinstance(layer,nn.Conv2d):\n",
        "                nn.init.normal_(layer.weight,mean=0,std=0.01)\n",
        "                nn.init.constant_(layer.bias,0)\n",
        "                # original paper = 1 for Conv2d layers 2nd, 4th, and 5th conv layers\n",
        "        nn.init.constant_(self.net[4].bias, 1)\n",
        "        nn.init.constant_(self.net[10].bias, 1)\n",
        "        nn.init.constant_(self.net[12].bias, 1)\n",
        "\n",
        "        def forward(self,x):\n",
        "            x=self.net(x)\n",
        "            x=x.view(-1,256*6*6)\n",
        "            return self.classifier(x)\n",
        "\n",
        "if __name__=='__main__':\n",
        "    alexnet=AlexNet(num_classes=NUM_ClASSES).to(device)\n",
        "    alexnet=torch.nn.parallel.DataParallel(alex_net,device_ids==DEVICE_IDS)\n",
        "    print(alexnet)\n",
        "    dataset=datasets.ImageFolder(TRAIN_IMG_DIR,transform.Compose([transform.CenterCrop(IMAGE_DIM),\n",
        "                                                                  transform.ToTensor,\n",
        "                                                                  transform.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225]),]))\n",
        "    dataloader=data.dataLoader(dataset,shuffle=true,num_workers=8,\n",
        "                               drop_last=True,batxh_size=BATCH_SIZE)\n",
        "    #create optimzer\n",
        "    optimizer=optim.Adam(params=alexnet.parameters(),Ir=0.0001)\n",
        "    Ir_schedular=optim.Ir_scheduler.StepLR(optimizer,step_size=30,gamma=0.1)\n",
        "\n",
        "    for epoch in range(NUM_EPOCHES):\n",
        "        Ir_scheduler.step()\n",
        "        for imgs,classes in dataloader:\n",
        "            imgs,classes=img.to(device),classes.to(device)\n",
        "            output=alexnet(imgs)\n",
        "            loss=F.cross_entropy(output,classes)\n",
        "            #u[date optimizer\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()]\n",
        "            # log the information and add to tensorboard\n",
        "            if total_steps % 10 == 0:\n",
        "                with torch.no_grad():\n",
        "                    _, preds = torch.max(output, 1)\n",
        "                    accuracy = torch.sum(preds == classes)\n",
        "\n",
        "                    print('Epoch: {} \\tStep: {} \\tLoss: {:.4f} \\tAcc: {}'\n",
        "                        .format(epoch + 1, total_steps, loss.item(), accuracy.item()))\n",
        "                    tbwriter.add_scalar('loss', loss.item(), total_steps)\n",
        "                    tbwriter.add_scalar('accuracy', accuracy.item(), total_steps)\n",
        "\n",
        "            # print out gradient values and parameter average values\n",
        "            if total_steps % 100 == 0:\n",
        "                with torch.no_grad():\n",
        "                    # print and save the grad of the parameters\n",
        "                    # also print and save parameter values\n",
        "                    print('*' * 10)\n",
        "                    for name, parameter in alexnet.named_parameters():\n",
        "                        if parameter.grad is not None:\n",
        "                            avg_grad = torch.mean(parameter.grad)\n",
        "                            print('\\t{} - grad_avg: {}'.format(name, avg_grad))\n",
        "                            tbwriter.add_scalar('grad_avg/{}'.format(name), avg_grad.item(), total_steps)\n",
        "                            tbwriter.add_histogram('grad/{}'.format(name),\n",
        "                                    parameter.grad.cpu().numpy(), total_steps)\n",
        "                        if parameter.data is not None:\n",
        "                            avg_weight = torch.mean(parameter.data)\n",
        "                            print('\\t{} - param_avg: {}'.format(name, avg_weight))\n",
        "                            tbwriter.add_histogram('weight/{}'.format(name),\n",
        "                                    parameter.data.cpu().numpy(), total_steps)\n",
        "                            tbwriter.add_scalar('weight_avg/{}'.format(name), avg_weight.item(), total_steps)\n",
        "\n",
        "            total_steps += 1\n",
        "\n",
        "        # save checkpoints\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, 'alexnet_states_e{}.pkl'.format(epoch + 1))\n",
        "        state = {\n",
        "            'epoch': epoch,\n",
        "            'total_steps': total_steps,\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'model': alexnet.state_dict(),\n",
        "            'seed': seed,\n",
        "        }\n",
        "        torch.save(state, checkpoint_path)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-1442ee118c18>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    nn.MaxPool2d(kernel_size=3, stride=2),\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}